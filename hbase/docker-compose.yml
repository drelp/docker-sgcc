version: '3.7'
services:

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    ports:
      - 50070:50070
    networks:
      - sgcc-network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: datanode
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:50070"
    env_file:
      - ./hadoop.env
    ports:
      - 50075:50075
    networks:
      - sgcc-network

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop2.7.4-java8
    container_name: resourcemanager
    environment:
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075"
    env_file:
      - ./hadoop.env
    ports:
      - 8088:8088
    networks:
      - sgcc-network

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop2.7.4-java8
    container_name: nodemanager
    environment:
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075 resourcemanager:8088"
    env_file:
      - ./hadoop.env
    ports:
      - 8042:8042
    networks:
      - sgcc-network

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop2.7.4-java8
    container_name: historyserver
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    environment:
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075 resourcemanager:8088"
    env_file:
      - ./hadoop.env
    ports:
      - 8188:8188
    networks:
      - sgcc-network

  zk1:
    # docker container所使用的docker image
    image: zookeeper
    hostname: zk1
    container_name: zk1
    # 配置docker container和宿主机的端口映射
    ports:
      - 2181:2181
      - 8080:8080
    # 配置docker container的环境变量
    environment:
      # 当前zk实例的id
      ZOO_MY_ID: 1
      # 整个zk集群的机器、端口列表
      ZOO_SERVERS: server.1=0.0.0.0:2888:3888;2181 server.2=zk2:2888:3888;2181 server.3=zk3:2888:3888;2181
    # 将docker container上的路径挂载到宿主机上 实现宿主机和docker container的数据共享
    volumes:
      - ./zk1/data:/data
      - ./zk1/datalog:/datalog
    # 当前docker container加入名为zk-net的隔离网络
    networks:
      - sgcc-network

  zk2:
    image: zookeeper
    hostname: zk2
    container_name: zk2
    ports:
      - 2182:2181
      - 8082:8080
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: server.1=zk1:2888:3888;2181 server.2=0.0.0.0:2888:3888;2181 server.3=zk3:2888:3888;2181
    volumes:
      - ./zk2/data:/data
      - ./zk2/datalog:/datalog
    networks:
      - sgcc-network

  zk3:
    image: zookeeper
    hostname: zk3
    container_name: zk3
    ports:
      - 2183:2181
      - 8083:8080
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: server.1=zk1:2888:3888;2181 server.2=zk2:2888:3888;2181 server.3=0.0.0.0:2888:3888;2181
    volumes:
      - ./zk3/data:/data
      - ./zk3/datalog:/datalog
    networks:
      - sgcc-network

  hbase-master:
    image: yiluxiangbei/hbase:2.4.9
    container_name: hbase-master
    environment:
      - HBASE_NODE_TYPE=master
    volumes:
      - "./conf:/opt/hbase/conf"
      - "./logs:/opt/hbase/logs"
      - /etc/localtime:/etc/localtime
    networks:
      - sgcc-network
    ports:
      - 16000:16000
      - 16010:16010
    restart: always

  hbase-region1:
    image: yiluxiangbei/hbase:2.4.9
    container_name: hbase-region1
    environment:
      - HBASE_NODE_TYPE=region
    volumes:
      - "./conf:/opt/hbase/conf"
      - "./logs:/opt/hbase/logs"
      - /etc/localtime:/etc/localtime
    networks:
      - sgcc-network
    ports:
      - 16020:16020
      - 16030:16030
    restart: always

  hbase-region2:
    image: yiluxiangbei/hbase:2.4.9
    container_name: hbase-region2
    environment:
      - HBASE_NODE_TYPE=region
    volumes:
      - "./conf:/opt/hbase/conf"
      - "./logs:/opt/hbase/logs"
      - /etc/localtime:/etc/localtime
    networks:
      - sgcc-network
    ports:
      - 16021:16020
      - 16031:16030
    restart: always

networks:
  sgcc-network:
    external: true

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver: